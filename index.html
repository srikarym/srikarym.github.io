<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srikar Yellapragada</title>
  
  <meta name="author" content="Srikar Yellapragada">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srikar Yellapragada</name>
              </p>
              <p>I am a PhD student in the Computer Science department at <a href="https://www.stonybrook.edu/">Stony Brook University</a>. My research is focused on Computer vision.  
                Before this, I was a Software Engineer at <a href="https://www.bloomberg.com/company/">Bloomberg LP</a>; our team built an ETL pipeline for the ingestion of third party data. 
              </p>
              <p>
                In 2020, I obtained my Masters in Computer Science from <a href="https://cims.nyu.edu/dynamic/">NYU</a>, where I worked with <a href="https://kyunghyuncho.me/">Kyunghyun Cho's</a>  group on <a href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">Similarity of Neural Networks</a>, and interned at <a href="https://www.ibm.com/watson-health">IBM Watson Health</a>.
              </p>
              <p>
                Previously, I obtained a B.Tech in Electrical Engineering from <a href="https://iith.ac.in/">Indian Institute of Technology, Hyderabad,</a>  where I worked with <a href="https://people.iith.ac.in/sumohana/">Sumohana Channappayya</a> on Image Processing. I interned at <a href="https://val.cds.iisc.ac.in/">Video Analytics Lab, IISc Bangalore</a>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:srikary@cs.stonybrook.edu">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1bRniIjx6jSsSGMWd-BdiloO-gO9LbonH/view?usp=sharing">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/srikarym/">Linkedin</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=DVMnHboAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ymsrikar">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srikarym/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/me.jpg" width="100%">
            </td>
          </tr>
        </tbody></table>

        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications / Pre-prints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		  <!-- Pub1 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href='images/isbi.png'><img src='images/isbi.png' width="180"></a>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.01648">
                <papertitle>Deep learning based detection of acute aortic syndrome in contrast CT images</papertitle>
              </a>
              <br>
              <strong>Manikanta Srikar Yellapragada</strong>, Yiting Xie, Benedikt Graf, David Richmond, Arun Krishnan, Arkadiusz Sitek
              <br>
                <em>In International Symposium on Biomedical Imaging (ISBI)</em>, 2020 
              <br>
              <a href="javascript:toggleBlock('abs_isbi')">abstract</a>
        /
              <a href="javascript:toggleBlock('bib_isbi')">bibtex</a>
        /
              <a href="https://arxiv.org/pdf/2004.01648.pdf">paper</a>
              <div id="abs_isbi">
                <p></p>
                Acute aortic syndrome (AAS) is a group of life threatening conditions of the aorta. We have developed an end-to-end automatic approach to detect AAS in computed tomography (CT) images. Our approach consists of two steps. At first, we extract N cross sections along the segmented aorta centerline for each CT scan. These cross sections are stacked together to form a new volume which is then classified using two different classifiers, a 3D convolutional neural network (3D CNN) and a multiple instance learning (MIL). We trained, validated, and compared two models on 2291 contrast CT volumes. We tested on a set aside cohort of 230 normal and 50 positive CT volumes. Our models detected AAS with an Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985 using 3DCNN and MIL, respectively.
                <p></p>
              </div>
              <div id="bib_isbi">
                <break-spaces>
                  <p></p>
                  @inproceedings{yellapragada2020deep,
                    title={Deep learning based detection of acute aortic syndrome in contrast CT images},<br>
                    author={Yellapragada, Manikanta Srikar and Xie, Yiting and Graf, Benedikt and Richmond, David and Krishnan, Arun and Sitek, Arkadiusz},<br>
                    booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},<br>
                    pages={1474--1477},<br>
                    year={2020},<br>
                    organization={IEEE}
                  }
                  <p></p>
                </break-spaces>
              </div>
            </td>
          </tr> 
      
      <!-- Pub2 -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href='images/midl.png'><img src='images/midl.png' width="180"></a>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">
            <papertitle>Automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study</papertitle>
          </a>
          <br>
          Luyao Shi, Deepta Rajan, Shafiq Abedin, <strong>Manikanta Srikar Yellapragada</strong>, David Beymer, Ehsan Dehghan
          <br>
            <em>In Medical Imaging with Deep Learning (MIDL)</em>, 2020 
          <br>
          <a  href="javascript:toggleBlock('abs_midl')">abstract</a>
    /
          <a href="javascript:toggleBlock('bib_midl')">bibtex</a>
    /
          <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">paper</a>
          <div id="abs_midl">
            <p></p>
            Pulmonary Embolism (PE) is a life-threatening disorder associated with high mortality and morbidity. Prompt diagnosis and immediate initiation of therapeutic action is important. We explored a deep learning model to detect PE on volumetric contrast-enhanced chest CT scans using a 2-stage training strategy. First, a residual convolutional neural network (ResNet) was trained using annotated 2D images. In addition to the classification loss, an attention loss was added during training to help the network focus attention on PE. Next, a recurrent network was used to scan sequentially through the features provided by the pre-trained ResNet to detect PE. This combination allows the network to be trained using both a limited and sparse set of pixel-level annotated images and a large number of easily obtainable patient-level image-label pairs. We used 1,670 sparsely annotated studies and more than 10,000 labeled studies in our training. On a test set with 2,160 patient studies, the proposed method achieved an area under the ROC curve (AUC) of 0.812. The proposed framework is also able to provide localized attention maps that indicate possible PE lesions, which could potentially help radiologists accelerate the diagnostic process.
            <p></p>
          </div>
          <div id="bib_midl">
            <break-spaces>
              <p></p>
              @inproceedings{shi2020automatic,<br>
                title={Automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study},<br>
                author={Shi, Luyao and Rajan, Deepta and Abedin, Shafiq and Yellapragada, Manikanta Srikar and Beymer, David and Dehghan, Ehsan},<br>
                booktitle={Medical Imaging with Deep Learning},<br>
                pages={743--754},<br>
                year={2020},<br>
                organization={PMLR}
                <p></p>
            </break-spaces>
          </div>
          
        </td>
      </tr> 

      <!-- Pub3 -->
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href='images/icip.png'><img src='images/icip.png' width="180"></a>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/1711.07245">
            <papertitle>Optical character recognition (ocr) for telugu: Database, algorithm and application</papertitle>
          </a>
          <br>
          Konkimalla Chandra Prakash*, <strong>Y.M.Srikar*</strong> , Gayam Trishal, Souraj Mandal, Sumohana S Channappayya
          <br>
            <em>In International Conference on Image Processing (ICIP)</em>, 2018 
          <br>
            <em>(* Equal Contribution)</em> 
          <br>
          <a  href="javascript:toggleBlock('abs_icip')">abstract</a>
    /
          <a href="javascript:toggleBlock('bib_icip')">bibtex</a>
    /
          <a href="https://arxiv.org/pdf/1711.07245.pdf">paper</a>
          <div id="abs_icip">
            <p></p>
            Telugu is a Dravidian language spoken by more than 80 million people worldwide. The optical character recognition (OCR) of the Telugu script has wide ranging applications including education, health-care, administration etc. The beautiful Telugu script however is very different from Germanic scripts like English and German. This makes the use of transfer learning of Germanic OCR solutions to Telugu a non-trivial task. To address the challenge of OCR for Telugu, we make three contributions in this work: (i) a database of Telugu characters, (ii) a deep learning based OCR algorithm, and (iii) a client server solution for the online deployment of the algorithm. For the benefit of the Telugu people and the research community, our code has been made freely available at this link.
            <p></p>
          </div>
          <div id="bib_icip">
            <break-spaces>
              <p></p>
              @inproceedings{prakash2018optical,<br>
                title={Optical character recognition (ocr) for telugu: Database, algorithm and application},<br>
                author={Prakash, Konkimalla Chandra and Srikar, YM and Trishal, Gayam and Mandal, Souraj and Channappayya, Sumohana S},<br>
                booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},<br>
                pages={3963--3967},<br>
                year={2018},<br>
                organization={IEEE}
              }
                <p></p>
            </break-spaces>
          </div>
          
        </td>
      </tr> 

    </tbody>
  </table>

		
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          
          <a href="https://jonbarron.info/">Credits</a>
          
          <br>
        </p>
      </td>
    </tr>
  </tbody></table>
        
<script>
  function toggleBlock(id) {
    var x = document.getElementById(id);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
  </script>
<script>
toggleBlock('abs_isbi');
toggleBlock('bib_isbi');
toggleBlock('abs_midl');
toggleBlock('bib_midl');
toggleBlock('abs_icip');
toggleBlock('bib_icip');
</script>
</body>

</html>
