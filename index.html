<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srikar Yellapragada</title>
  
  <meta name="author" content="Srikar Yellapragada">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Srikar Yellapragada</name>
              </p>
              <img src="images/me.png" class="img_me">
              <p>I am a PhD student in the Computer Science department at <a href="https://www.stonybrook.edu/">Stony Brook University</a>, advised by <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a> My research is focused on Computer vision.  
                Before this, I was a Software Engineer at <a href="https://www.bloomberg.com/company/">Bloomberg LP</a>; our team built an ETL pipeline for the ingestion of third party data. 
              </p>
              <p>
                In 2020, I obtained my Masters in Computer Science from <a href="https://cims.nyu.edu/dynamic/">NYU</a>, where I worked with <a href="https://kyunghyuncho.me/">Kyunghyun Cho's</a>  group on <a href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">Similarity of Neural Networks</a>, and interned at <a href="https://www.ibm.com/watson-health">IBM Watson Health</a>.
              </p>
              <p>
                Previously, I obtained a B.Tech in Electrical Engineering from <a href="https://iith.ac.in/">Indian Institute of Technology, Hyderabad,</a>  where I worked with <a href="https://people.iith.ac.in/sumohana/">Sumohana Channappayya</a> on Image Processing. I interned at <a href="https://val.cds.iisc.ac.in/">Video Analytics Lab, IISc Bangalore</a>. 
              </p>
              <p>In my free time, I enjoy playing video games. I'm a big fan of strategy games, such as Dota 2 and Age of Empires 2. </p>
              <br>
              <p style="text-align:center">
                <a href="mailto:srikary@cs.stonybrook.edu">Email</a> &nbsp/&nbsp
                <a href="https://drive.google.com/file/d/1bRniIjx6jSsSGMWd-BdiloO-gO9LbonH/view?usp=sharing">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/srikarym/">Linkedin</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=DVMnHboAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ymsrikar">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/srikarym/">Github</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications / Pre-prints</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href='images/diff_cond.png'><img src='images/diff_cond.png' class="img_pub"></a>
            <a href="https://arxiv.org/abs/2306.01900">
              <papertitle>Conditional Generation from Unconditional Diffusion Models using Denoiser Representations
              </papertitle>
            </a>
            <br>
            Alexandros Graikos, <strong>Srikar Yellapragada</strong>, Dimitris Samaras
            <br>
              <em>In British Machine Vision Conference (BMVC)</em>, 2023 
            <br>
            <a href="javascript:toggleBlock('abs_arxiv_6_23')">abstract</a>
      /
            <a href="javascript:toggleBlock('bib_arxiv_6_23')">bibtex</a>
      /
            <a href="https://arxiv.org/pdf/2306.01900.pdf">paper</a>

            <div id="abs_arxiv_6_23">
              <p></p>
              Denoising diffusion models have gained popularity as a generative modeling technique for producing high-quality and diverse images. Applying these models to downstream tasks requires conditioning, which can take the form of text, class labels, or other forms of guidance. However, providing conditioning information to these models can be challenging, particularly when annotations are scarce or imprecise. In this paper, we propose adapting pre-trained unconditional diffusion models to new conditions using the learned internal representations of the denoiser network. We demonstrate the effectiveness of our approach on various conditional generation tasks, including attribute-conditioned generation and mask-conditioned generation. Additionally, we show that augmenting the Tiny ImageNet training set with synthetic images generated by our approach improves the classification accuracy of ResNet baselines by up to 8%. Our approach provides a powerful and flexible way to adapt diffusion models to new conditions and generate high-quality augmented data for various conditional generation tasks.
              <p></p>
            </div>
            <div id="bib_arxiv_6_23" class="bib" >
              <break-spaces>
                <p></p>

                @misc{graikos2023conditional, <br>
                  title={Conditional Generation from Unconditional Diffusion Models using Denoiser Representations},  <br>
                  author={Alexandros Graikos and Srikar Yellapragada and Dimitris Samaras}, <br>
                  year={2023}, <br>
                  eprint={2306.01900}, <br>
                  archivePrefix={arXiv}, <br>
                  primaryClass={cs.CV}
            }
                <p></p>
              </break-spaces>
            </div>
          </td>
        </tr> 
      
      
      <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href='images/cctv_gun.jpg'><img src='images/cctv_gun.jpg' class="img_pub"></a>
            <a href="https://arxiv.org/abs/2303.10703">
              <papertitle>CCTV-Gun: Benchmarking Handgun Detection in CCTV Images</papertitle>
            </a>
            <br>
            <strong>Srikar Yellapragada</strong>, Zhenghong Li, Kevin Bhadresh Doshi, Purva Makarand Mhasakar, Heng Fan, Jie Wei, Erik Blasch, Haibin Ling
            <br>
              <em>arXiv</em>, 2023
            <br> 
            <a href="javascript:toggleBlock('abs_arxiv_3_23')">abstract</a>
      /
            <a href="javascript:toggleBlock('bib_arxiv_3_23')">bibtex</a>
      /
            <a href="https://arxiv.org/pdf/2303.10703">paper</a>
      /
            <a href="https://github.com/srikarym/CCTV-Gun">code</a>

            <div id="abs_arxiv_3_23">
              <p></p>
              Gun violence is a critical security problem, and it is imperative for the computer vision community to develop effective gun detection algorithms for real-world scenarios, particularly in Closed Circuit Television (CCTV) surveillance data. Despite significant progress in visual object detection, detecting guns in real-world CCTV images remains a challenging and under-explored task. Firearms, especially handguns, are typically very small in size, non-salient in appearance, and often severely occluded or indistinguishable from other small objects. Additionally, the lack of principled benchmarks and difficulty collecting relevant datasets further hinder algorithmic development. In this paper, we present a meticulously crafted and annotated benchmark, called \textbf{CCTV-Gun}, which addresses the challenges of detecting handguns in real-world CCTV images. Our contribution is three-fold. Firstly, we carefully select and analyze real-world CCTV images from three datasets, manually annotate handguns and their holders, and assign each image with relevant challenge factors such as blur and occlusion. Secondly, we propose a new cross-dataset evaluation protocol in addition to the standard intra-dataset protocol, which is vital for gun detection in practical settings. Finally, we comprehensively evaluate both classical and state-of-the-art object detection algorithms, providing an in-depth analysis of their generalizing abilities. The benchmark will facilitate further research and development on this topic and ultimately enhance security. Code, annotations, and trained models are available at https://github.com/srikarym/CCTV-Gun
              <p></p>
            </div>
            <div id="bib_arxiv_3_23" class="bib" >
              <break-spaces>
                <p></p>
                @misc{yellapragada2023cctvgun, <br>
                  title={CCTV-Gun: Benchmarking Handgun Detection in CCTV Images}, <br>
                  author={Srikar Yellapragada and Zhenghong Li and Kevin Bhadresh Doshi and Purva Makarand Mhasakar and Heng Fan and Jie Wei and Erik Blasch and Haibin Ling},<br>
                  year={2023},<br>
                  eprint={2303.10703},<br>
                  archivePrefix={arXiv},<br>
                  primaryClass={cs.CV}
            }
                <p></p>
              </break-spaces>
            </div>
          </td>
        </tr> 
    
    <!-- ms_thesis -->
    <tr>
          
      <td style="padding:10px;width:100%;vertical-align:middle">
        <a href='images/thesis.png'><img src='images/thesis.png' class="img_pub"></a>
        <a href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">
          <papertitle>Are the proposed similarity metrics also a measure of functional similarity?</papertitle>
        </a>
        <br>
        <strong>Manikanta Srikar Yellapragada</strong>
        <br>
          <em>Masters Thesis</em>, 2020 
        <br>
        <a href="javascript:toggleBlock('abs_thesis')">abstract</a>
  /
        <a href="javascript:toggleBlock('bib_thesis')">bibtex</a>
  /
        <a href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">paper</a>
        <div id="abs_thesis">
          <p></p>
          A recent body of work attempts to understand the behavior and training dynamics of neural
            networks by analyzing intermediate representations and designing metrics to define the similarity
            between those representations. We observe that the representations of the last layer could be
            thought of as the functional output of the model up to that point. In this work, we investigate
            if the similarity between these representations can be considered a stand-in for the similarity
            of the networks‚Äô output functions. This can have an impact for many downstream tasks, but
            we specifically analyze it in the context of transfer learning. Consequently, we perform a series
            of experiments to understand the relationship between the representational similarity and the
            functional similarity of neural networks. We show in two ways that the leading metric for
            representational similarity, CKA, does not bear a strict relationship with functional similarity
          <p></p>
        </div>
        <div id="bib_thesis">
          <break-spaces>
            <p></p>
            @misc{yellapragada2020proposed,
              title={Are the proposed similarity metrics also a measure of functional similarity?}, <br>
              author={Yellapragada, Manikanta Srikar}, <br>
              journal={https://cs.nyu.edu/media/publications/Yellapragada\_\_Manikanta\_Srikar\_-\_MS\_Thesis.pdf}, <br>
              year={2020}, <br>
              publisher={New York University}
            }
            <p></p>
          </break-spaces>
        </div>
      </td>
    </tr>
    
    <!-- Pub1 -->
        <tr>
          
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href='images/isbi.png'><img src='images/isbi.png' class="img_pub"></a>
            <a href="https://arxiv.org/abs/2004.01648">
              <papertitle>Deep learning based detection of acute aortic syndrome in contrast CT images</papertitle>
            </a>
            <br>
            <strong>Manikanta Srikar Yellapragada</strong>, Yiting Xie, Benedikt Graf, David Richmond, Arun Krishnan, Arkadiusz Sitek
            <br>
              <em>In International Symposium on Biomedical Imaging (ISBI)</em>, 2020 
            <br>
            <a href="javascript:toggleBlock('abs_isbi')">abstract</a>
      /
            <a href="javascript:toggleBlock('bib_isbi')">bibtex</a>
      /
            <a href="https://arxiv.org/pdf/2004.01648.pdf">paper</a>
            <div id="abs_isbi">
              <p></p>
              Acute aortic syndrome (AAS) is a group of life threatening conditions of the aorta. We have developed an end-to-end automatic approach to detect AAS in computed tomography (CT) images. Our approach consists of two steps. At first, we extract N cross sections along the segmented aorta centerline for each CT scan. These cross sections are stacked together to form a new volume which is then classified using two different classifiers, a 3D convolutional neural network (3D CNN) and a multiple instance learning (MIL). We trained, validated, and compared two models on 2291 contrast CT volumes. We tested on a set aside cohort of 230 normal and 50 positive CT volumes. Our models detected AAS with an Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985 using 3DCNN and MIL, respectively.
              <p></p>
            </div>
            <div id="bib_isbi">
              <break-spaces>
                <p></p>
                @inproceedings{yellapragada2020deep,
                  title={Deep learning based detection of acute aortic syndrome in contrast CT images},<br>
                  author={Yellapragada, Manikanta Srikar and Xie, Yiting and Graf, Benedikt and Richmond, David and Krishnan, Arun and Sitek, Arkadiusz},<br>
                  booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},<br>
                  pages={1474--1477},<br>
                  year={2020},<br>
                  organization={IEEE}
                }
                <p></p>
              </break-spaces>
            </div>
          </td>
        </tr> 
    
    <!-- Pub2 -->
    <tr>
      <td style="padding:10px;width:100%;vertical-align:middle">
        <a href='images/midl.png'><img src='images/midl.png' height="80px" class="img_pub"></a>
        <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">
          <papertitle>Automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study</papertitle>
        </a>
        <br>
        Luyao Shi, Deepta Rajan, Shafiq Abedin, <strong>Manikanta Srikar Yellapragada</strong>, David Beymer, Ehsan Dehghan
        <br>
          <em>In Medical Imaging with Deep Learning (MIDL)</em>, 2020 
        <br>
        <a  href="javascript:toggleBlock('abs_midl')">abstract</a>
  /
        <a href="javascript:toggleBlock('bib_midl')">bibtex</a>
  /
        <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">paper</a>
        <div id="abs_midl">
          <p></p>
          Pulmonary Embolism (PE) is a life-threatening disorder associated with high mortality and morbidity. Prompt diagnosis and immediate initiation of therapeutic action is important. We explored a deep learning model to detect PE on volumetric contrast-enhanced chest CT scans using a 2-stage training strategy. First, a residual convolutional neural network (ResNet) was trained using annotated 2D images. In addition to the classification loss, an attention loss was added during training to help the network focus attention on PE. Next, a recurrent network was used to scan sequentially through the features provided by the pre-trained ResNet to detect PE. This combination allows the network to be trained using both a limited and sparse set of pixel-level annotated images and a large number of easily obtainable patient-level image-label pairs. We used 1,670 sparsely annotated studies and more than 10,000 labeled studies in our training. On a test set with 2,160 patient studies, the proposed method achieved an area under the ROC curve (AUC) of 0.812. The proposed framework is also able to provide localized attention maps that indicate possible PE lesions, which could potentially help radiologists accelerate the diagnostic process.
          <p></p>
        </div>
        <div id="bib_midl">
          <break-spaces>
            <p></p>
            @inproceedings{shi2020automatic,<br>
              title={Automatic diagnosis of pulmonary embolism using an attention-guided framework: a large-scale study},<br>
              author={Shi, Luyao and Rajan, Deepta and Abedin, Shafiq and Yellapragada, Manikanta Srikar and Beymer, David and Dehghan, Ehsan},<br>
              booktitle={Medical Imaging with Deep Learning},<br>
              pages={743--754},<br>
              year={2020},<br>
              organization={PMLR}
              <p></p>
          </break-spaces>
        </div>
        
      </td>
    </tr> 

    <!-- Pub3 -->
    <tr>
      <td style="padding:10px;width:100%;vertical-align:middle">
        <a href='images/icip.png'><img src='images/icip.png' class="img_pub"></a>
        <a href="https://arxiv.org/abs/1711.07245">
          <papertitle>Optical character recognition (ocr) for telugu: Database, algorithm and application</papertitle>
        </a>
        <br>
        Konkimalla Chandra Prakash, <strong>Y.M.Srikar</strong> , Gayam Trishal, Souraj Mandal, Sumohana S Channappayya
        <br>
          <em>In International Conference on Image Processing (ICIP)</em>, 2018
        <br> 
        <a  href="javascript:toggleBlock('abs_icip')">abstract</a>
  /
        <a href="javascript:toggleBlock('bib_icip')">bibtex</a>
  /
        <a href="https://arxiv.org/pdf/1711.07245.pdf">paper</a>
        <div id="abs_icip">
          <p></p>
          Telugu is a Dravidian language spoken by more than 80 million people worldwide. The optical character recognition (OCR) of the Telugu script has wide ranging applications including education, health-care, administration etc. The beautiful Telugu script however is very different from Germanic scripts like English and German. This makes the use of transfer learning of Germanic OCR solutions to Telugu a non-trivial task. To address the challenge of OCR for Telugu, we make three contributions in this work: (i) a database of Telugu characters, (ii) a deep learning based OCR algorithm, and (iii) a client server solution for the online deployment of the algorithm. For the benefit of the Telugu people and the research community, our code has been made freely available at this link.
          <p></p>
        </div>
        <div id="bib_icip">
          <break-spaces>
            <p></p>
            @inproceedings{prakash2018optical,<br>
              title={Optical character recognition (ocr) for telugu: Database, algorithm and application},<br>
              author={Prakash, Konkimalla Chandra and Srikar, YM and Trishal, Gayam and Mandal, Souraj and Channappayya, Sumohana S},<br>
              booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},<br>
              pages={3963--3967},<br>
              year={2018},<br>
              organization={IEEE}
            }
              <p></p>
          </break-spaces>
        </div>
        
      </td>
    </tr> 

  </tbody>
</table>

  
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:right;font-size:small;">
        
        <a href="https://jonbarron.info/">Credits</a>
        
        <br>
      </p>
    </td>
  </tr>
</tbody></table>
      
<script>
function toggleBlock(id) {
  var x = document.getElementById(id);
  if (x.style.display === "none") {
    x.style.display = "flex";
  } else {
    x.style.display = "none";
  }
}
</script>
<script>
toggleBlock('abs_arxiv_6_23');
toggleBlock('bib_arxiv_6_23');
toggleBlock('abs_arxiv_3_23');
toggleBlock('bib_arxiv_3_23');
toggleBlock('abs_thesis');
toggleBlock('bib_thesis');
toggleBlock('abs_isbi');
toggleBlock('bib_isbi');
toggleBlock('abs_midl');
toggleBlock('bib_midl');
toggleBlock('abs_icip');
toggleBlock('bib_icip');
</script>
</body>

</html>