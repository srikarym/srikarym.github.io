<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Srikar Yellapragada</title>

  <meta name="author" content="Srikar Yellapragada">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Srikar Yellapragada</name>
                  </p>
                  <img src="images/me.JPG" class="img_me">
                  <p>I am a PhD student in the Computer Science department at <a
                      href="https://www.stonybrook.edu/">Stony Brook University</a>, advised by <a
                      href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a> My research is focused on
                    Computer vision.
                    Before this, I was a Software Engineer at <a href="https://www.bloomberg.com/company/">Bloomberg
                      LP</a>; our team built an ETL pipeline for the ingestion of third party data.
                  </p>
                  <p>
                    In 2020, I obtained my Masters in Computer Science from <a
                      href="https://cims.nyu.edu/dynamic/">NYU</a>, where I worked with <a
                      href="https://kyunghyuncho.me/">Kyunghyun Cho's</a> group on <a
                      href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">Similarity
                      of Neural Networks</a>, and interned at <a href="https://www.ibm.com/watson-health">IBM Watson
                      Health</a>.
                  </p>
                  <p>
                    Previously, I obtained a B.Tech in Electrical Engineering from <a href="https://iith.ac.in/">Indian
                      Institute of Technology, Hyderabad,</a> where I worked with <a
                      href="https://people.iith.ac.in/sumohana/">Sumohana Channappayya</a> on Image Processing. I
                    interned at <a href="https://val.cds.iisc.ac.in/">Video Analytics Lab, IISc Bangalore</a>.
                  </p>
                  <p>In my free time, I enjoy playing video games. I'm a big fan of strategy games, such as Dota 2 and
                    Age of Empires 2. </p>
                  <br>
                  <p style="text-align:center">
                    <a href="mailto:srikary@cs.stonybrook.edu">Email</a> &nbsp/&nbsp
                    <a href="https://drive.google.com/file/d/1bRniIjx6jSsSGMWd-BdiloO-gO9LbonH/view?usp=sharing">CV</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/srikarym/">Linkedin</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=DVMnHboAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/ymsrikar">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/srikarym/">Github</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications / Pre-prints</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/large_img.png'><img src='images/large_img.png' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/2312.07330">
                    <papertitle>Learned representation-guided diffusion models for large-image generation
                    </papertitle>
                  </a>
                  <br>
                  Alexandros Graikos*, <strong>Srikar Yellapragada*</strong>,  Minh-Quan Le, Saarthak Kapse, Prateek
                      Prasanna, Joel Saltz, Dimitris Samaras
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="javascript:toggleBlock('abs_arxiv_12_12')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_arxiv_12_12')">bibtex</a>
                  /
                  <a href="https://arxiv.org/pdf/2312.07330.pdf">paper</a>

                  <div id="abs_arxiv_12_12">
                    <p></p>
                    To synthesize high-fidelity samples, diffusion models typically require auxiliary data to guide the
                    generation process. However, it is impractical to procure the painstaking patch-level annotation
                    effort required in specialized domains like histopathology and satellite imagery; it is often
                    performed by domain experts and involves hundreds of millions of patches. Modern-day self-supervised
                    learning (SSL) representations encode rich semantic and visual information. In this paper, we posit
                    that such representations are expressive enough to act as proxies to fine-grained human labels. We
                    introduce a novel approach that trains diffusion models conditioned on embeddings from SSL. Our
                    diffusion models successfully project these features back to high-quality histopathology and remote
                    sensing images. In addition, we construct larger images by assembling spatially consistent patches
                    inferred from SSL embeddings, preserving long-range dependencies. Augmenting real data by generating
                    variations of real images improves downstream classifier accuracy for patch-level and larger,
                    image-scale classification tasks. Our models are effective even on datasets not encountered during
                    training, demonstrating their robustness and generalizability. Generating images from learned
                    embeddings is agnostic to the source of the embeddings. The SSL embeddings used to generate a large
                    image can either be extracted from a reference image, or sampled from an auxiliary model conditioned
                    on any related modality (e.g. class labels, text, genomic data). As proof of concept, we introduce
                    the text-to-large image synthesis paradigm where we successfully synthesize large pathology and
                    satellite images out of text descriptions.
                    <p></p>
                  </div>
                  <div id="bib_arxiv_12_12" class="bib">
                    <break-spaces>
                      <p></p>

                      @misc{graikos2023learned, <br>
                      title={Learned representation-guided diffusion models for large-image generation}, <br>
                      author={Alexandros Graikos and Srikar Yellapragada and Minh-Quan Le and Saarthak Kapse and Prateek
                      Prasanna and Joel Saltz and Dimitris Samaras}, <br>
                      year={2023}, <br>
                      eprint={2312.07330}, <br>
                      archivePrefix={arXiv}, <br>
                      primaryClass={cs.CV}
                      }
                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/pathldm.png'><img src='images/pathldm.png' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/2309.00748">
                    <papertitle>PathLDM: Text conditioned Latent Diffusion Model for Histopathology
                    </papertitle>
                  </a>
                  <br>
                  <strong>Srikar Yellapragada</strong>, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz,
                  Dimitris Samaras
                  <br>
                  <em>In Winter Conference on Applications of Computer Vision (WACV)</em>, 2024
                  <br>
                  <a href="javascript:toggleBlock('abs_arxiv_9_23')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_arxiv_9_23')">bibtex</a>
                  /
                  <a href="https://arxiv.org/pdf/2309.00748.pdf">paper</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/PathLDM">code</a>

                  <div id="abs_arxiv_9_23">
                    <p></p>
                    To achieve high-quality results, diffusion models must be trained on large datasets. This can be
                    notably prohibitive for models in specialized domains, such as computational pathology. Conditioning
                    on labeled data is known to help in data-efficient model training. Therefore, histopathology
                    reports, which are rich in valuable clinical information, are an ideal choice as guidance for a
                    histopathology generative model. In this paper, we introduce PathLDM, the first text-conditioned
                    Latent Diffusion Model tailored for generating high-quality histopathology images. Leveraging the
                    rich contextual information provided by pathology text reports, our approach fuses image and textual
                    data to enhance the generation process. By utilizing GPT's capabilities to distill and summarize
                    complex text reports, we establish an effective conditioning mechanism. Through strategic
                    conditioning and necessary architectural enhancements, we achieved a SoTA FID score of 7.64 for
                    text-to-image generation on the TCGA-BRCA dataset, significantly outperforming the closest
                    text-conditioned competitor with FID 30.1.
                    <p></p>
                  </div>
                  <div id="bib_arxiv_9_23" class="bib">
                    <break-spaces>
                      <p></p>

                      @misc{yellapragada2023pathldm,<br>
                      title={PathLDM: Text conditioned Latent Diffusion Model for Histopathology}, <br>
                      author={Srikar Yellapragada and Alexandros Graikos and Prateek Prasanna and Tahsin Kurc and Joel
                      Saltz and Dimitris Samaras},<br>
                      year={2023},<br>
                      eprint={2309.00748},<br>
                      archivePrefix={arXiv},<br>
                      primaryClass={cs.CV}
                      }
                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/diff_cond.png'><img src='images/diff_cond.png' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/2306.01900">
                    <papertitle>Conditional Generation from Unconditional Diffusion Models using Denoiser
                      Representations
                    </papertitle>
                  </a>
                  <br>
                  Alexandros Graikos, <strong>Srikar Yellapragada</strong>, Dimitris Samaras
                  <br>
                  <em>In British Machine Vision Conference (BMVC)</em>, 2023
                  <br>
                  <a href="javascript:toggleBlock('abs_arxiv_6_23')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_arxiv_6_23')">bibtex</a>
                  /
                  <a href="https://papers.bmvc2023.org/0478.pdf">paper</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/fewshot-conditional-diffusion">code</a>
                  /
                  <a href="https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0478_poster.pdf">poster</a>

                  <div id="abs_arxiv_6_23">
                    <p></p>
                    Denoising diffusion models have gained popularity as a generative modeling technique for producing
                    high-quality and diverse images. Applying these models to downstream tasks requires conditioning,
                    which can take the form of text, class labels, or other forms of guidance. However, providing
                    conditioning information to these models can be challenging, particularly when annotations are
                    scarce or imprecise. In this paper, we propose adapting pre-trained unconditional diffusion models
                    to new conditions using the learned internal representations of the denoiser network. We demonstrate
                    the effectiveness of our approach on various conditional generation tasks, including
                    attribute-conditioned generation and mask-conditioned generation. Additionally, we show that
                    augmenting the Tiny ImageNet training set with synthetic images generated by our approach improves
                    the classification accuracy of ResNet baselines by up to 8%. Our approach provides a powerful and
                    flexible way to adapt diffusion models to new conditions and generate high-quality augmented data
                    for various conditional generation tasks.
                    <p></p>
                  </div>
                  <div id="bib_arxiv_6_23" class="bib">
                    <break-spaces>
                      <p></p>

                      @inproceedings{Graikos_2023_BMVC,
                      author = {Alexandros Graikos and Srikar Yellapragada and Dimitris Samaras},
                      title = {Conditional Generation from Pre-Trained Diffusion Models using Denoiser Representations},
                      booktitle = {34th British Machine Vision Conference 2023, {BMVC} 2023, Aberdeen, UK, November
                      20-24, 2023},
                      publisher = {BMVA},
                      year = {2023},
                      url = {https://papers.bmvc2023.org/0478.pdf}
                      }

                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>


              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/cctv_gun.jpg'><img src='images/cctv_gun.jpg' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/2303.10703">
                    <papertitle>CCTV-Gun: Benchmarking Handgun Detection in CCTV Images</papertitle>
                  </a>
                  <br>
                  <strong>Srikar Yellapragada</strong>, Zhenghong Li, Kevin Bhadresh Doshi, Purva Makarand Mhasakar,
                  Heng Fan, Jie Wei, Erik Blasch, Haibin Ling
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="javascript:toggleBlock('abs_arxiv_3_23')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_arxiv_3_23')">bibtex</a>
                  /
                  <a href="https://arxiv.org/pdf/2303.10703">paper</a>
                  /
                  <a href="https://github.com/srikarym/CCTV-Gun">code</a>

                  <div id="abs_arxiv_3_23">
                    <p></p>
                    Gun violence is a critical security problem, and it is imperative for the computer vision community
                    to develop effective gun detection algorithms for real-world scenarios, particularly in Closed
                    Circuit Television (CCTV) surveillance data. Despite significant progress in visual object
                    detection, detecting guns in real-world CCTV images remains a challenging and under-explored task.
                    Firearms, especially handguns, are typically very small in size, non-salient in appearance, and
                    often severely occluded or indistinguishable from other small objects. Additionally, the lack of
                    principled benchmarks and difficulty collecting relevant datasets further hinder algorithmic
                    development. In this paper, we present a meticulously crafted and annotated benchmark, called
                    \textbf{CCTV-Gun}, which addresses the challenges of detecting handguns in real-world CCTV images.
                    Our contribution is three-fold. Firstly, we carefully select and analyze real-world CCTV images from
                    three datasets, manually annotate handguns and their holders, and assign each image with relevant
                    challenge factors such as blur and occlusion. Secondly, we propose a new cross-dataset evaluation
                    protocol in addition to the standard intra-dataset protocol, which is vital for gun detection in
                    practical settings. Finally, we comprehensively evaluate both classical and state-of-the-art object
                    detection algorithms, providing an in-depth analysis of their generalizing abilities. The benchmark
                    will facilitate further research and development on this topic and ultimately enhance security.
                    Code, annotations, and trained models are available at https://github.com/srikarym/CCTV-Gun
                    <p></p>
                  </div>
                  <div id="bib_arxiv_3_23" class="bib">
                    <break-spaces>
                      <p></p>
                      @misc{yellapragada2023cctvgun, <br>
                      title={CCTV-Gun: Benchmarking Handgun Detection in CCTV Images}, <br>
                      author={Srikar Yellapragada and Zhenghong Li and Kevin Bhadresh Doshi and Purva Makarand Mhasakar
                      and Heng Fan and Jie Wei and Erik Blasch and Haibin Ling},<br>
                      year={2023},<br>
                      eprint={2303.10703},<br>
                      archivePrefix={arXiv},<br>
                      primaryClass={cs.CV}
                      }
                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>

              <!-- ms_thesis -->
              <tr>

                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/thesis.png'><img src='images/thesis.png' class="img_pub"></a>
                  <a href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">
                    <papertitle>Are the proposed similarity metrics also a measure of functional similarity?
                    </papertitle>
                  </a>
                  <br>
                  <strong>Manikanta Srikar Yellapragada</strong>
                  <br>
                  <em>Masters Thesis</em>, 2020
                  <br>
                  <a href="javascript:toggleBlock('abs_thesis')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_thesis')">bibtex</a>
                  /
                  <a
                    href="https://cs.nyu.edu/media/publications/Yellapragada__Manikanta_Srikar_-_MS_Thesis.pdf">paper</a>
                  <div id="abs_thesis">
                    <p></p>
                    A recent body of work attempts to understand the behavior and training dynamics of neural
                    networks by analyzing intermediate representations and designing metrics to define the similarity
                    between those representations. We observe that the representations of the last layer could be
                    thought of as the functional output of the model up to that point. In this work, we investigate
                    if the similarity between these representations can be considered a stand-in for the similarity
                    of the networks‚Äô output functions. This can have an impact for many downstream tasks, but
                    we specifically analyze it in the context of transfer learning. Consequently, we perform a series
                    of experiments to understand the relationship between the representational similarity and the
                    functional similarity of neural networks. We show in two ways that the leading metric for
                    representational similarity, CKA, does not bear a strict relationship with functional similarity
                    <p></p>
                  </div>
                  <div id="bib_thesis">
                    <break-spaces>
                      <p></p>
                      @misc{yellapragada2020proposed,
                      title={Are the proposed similarity metrics also a measure of functional similarity?}, <br>
                      author={Yellapragada, Manikanta Srikar}, <br>
                      journal={https://cs.nyu.edu/media/publications/Yellapragada\_\_Manikanta\_Srikar\_-\_MS\_Thesis.pdf},
                      <br>
                      year={2020}, <br>
                      publisher={New York University}
                      }
                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>

              <!-- Pub1 -->
              <tr>

                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/isbi.png'><img src='images/isbi.png' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/2004.01648">
                    <papertitle>Deep learning based detection of acute aortic syndrome in contrast CT images
                    </papertitle>
                  </a>
                  <br>
                  <strong>Manikanta Srikar Yellapragada</strong>, Yiting Xie, Benedikt Graf, David Richmond, Arun
                  Krishnan, Arkadiusz Sitek
                  <br>
                  <em>In International Symposium on Biomedical Imaging (ISBI)</em>, 2020
                  <br>
                  <a href="javascript:toggleBlock('abs_isbi')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_isbi')">bibtex</a>
                  /
                  <a href="https://arxiv.org/pdf/2004.01648.pdf">paper</a>
                  <div id="abs_isbi">
                    <p></p>
                    Acute aortic syndrome (AAS) is a group of life threatening conditions of the aorta. We have
                    developed an end-to-end automatic approach to detect AAS in computed tomography (CT) images. Our
                    approach consists of two steps. At first, we extract N cross sections along the segmented aorta
                    centerline for each CT scan. These cross sections are stacked together to form a new volume which is
                    then classified using two different classifiers, a 3D convolutional neural network (3D CNN) and a
                    multiple instance learning (MIL). We trained, validated, and compared two models on 2291 contrast CT
                    volumes. We tested on a set aside cohort of 230 normal and 50 positive CT volumes. Our models
                    detected AAS with an Area under Receiver Operating Characteristic curve (AUC) of 0.965 and 0.985
                    using 3DCNN and MIL, respectively.
                    <p></p>
                  </div>
                  <div id="bib_isbi">
                    <break-spaces>
                      <p></p>
                      @inproceedings{yellapragada2020deep,
                      title={Deep learning based detection of acute aortic syndrome in contrast CT images},<br>
                      author={Yellapragada, Manikanta Srikar and Xie, Yiting and Graf, Benedikt and Richmond, David and
                      Krishnan, Arun and Sitek, Arkadiusz},<br>
                      booktitle={2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)},<br>
                      pages={1474--1477},<br>
                      year={2020},<br>
                      organization={IEEE}
                      }
                      <p></p>
                    </break-spaces>
                  </div>
                </td>
              </tr>

              <!-- Pub2 -->
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/midl.png'><img src='images/midl.png' height="80px" class="img_pub"></a>
                  <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">
                    <papertitle>Automatic diagnosis of pulmonary embolism using an attention-guided framework: a
                      large-scale study</papertitle>
                  </a>
                  <br>
                  Luyao Shi, Deepta Rajan, Shafiq Abedin, <strong>Manikanta Srikar Yellapragada</strong>, David Beymer,
                  Ehsan Dehghan
                  <br>
                  <em>In Medical Imaging with Deep Learning (MIDL)</em>, 2020
                  <br>
                  <a href="javascript:toggleBlock('abs_midl')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_midl')">bibtex</a>
                  /
                  <a href="http://proceedings.mlr.press/v121/shi20a/shi20a.pdf">paper</a>
                  <div id="abs_midl">
                    <p></p>
                    Pulmonary Embolism (PE) is a life-threatening disorder associated with high mortality and morbidity.
                    Prompt diagnosis and immediate initiation of therapeutic action is important. We explored a deep
                    learning model to detect PE on volumetric contrast-enhanced chest CT scans using a 2-stage training
                    strategy. First, a residual convolutional neural network (ResNet) was trained using annotated 2D
                    images. In addition to the classification loss, an attention loss was added during training to help
                    the network focus attention on PE. Next, a recurrent network was used to scan sequentially through
                    the features provided by the pre-trained ResNet to detect PE. This combination allows the network to
                    be trained using both a limited and sparse set of pixel-level annotated images and a large number of
                    easily obtainable patient-level image-label pairs. We used 1,670 sparsely annotated studies and more
                    than 10,000 labeled studies in our training. On a test set with 2,160 patient studies, the proposed
                    method achieved an area under the ROC curve (AUC) of 0.812. The proposed framework is also able to
                    provide localized attention maps that indicate possible PE lesions, which could potentially help
                    radiologists accelerate the diagnostic process.
                    <p></p>
                  </div>
                  <div id="bib_midl">
                    <break-spaces>
                      <p></p>
                      @inproceedings{shi2020automatic,<br>
                      title={Automatic diagnosis of pulmonary embolism using an attention-guided framework: a
                      large-scale study},<br>
                      author={Shi, Luyao and Rajan, Deepta and Abedin, Shafiq and Yellapragada, Manikanta Srikar and
                      Beymer, David and Dehghan, Ehsan},<br>
                      booktitle={Medical Imaging with Deep Learning},<br>
                      pages={743--754},<br>
                      year={2020},<br>
                      organization={PMLR}
                      <p></p>
                    </break-spaces>
                  </div>

                </td>
              </tr>

              <!-- Pub3 -->
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href='images/icip.png'><img src='images/icip.png' class="img_pub"></a>
                  <a href="https://arxiv.org/abs/1711.07245">
                    <papertitle>Optical character recognition (ocr) for telugu: Database, algorithm and application
                    </papertitle>
                  </a>
                  <br>
                  Konkimalla Chandra Prakash, <strong>Y.M.Srikar</strong> , Gayam Trishal, Souraj Mandal, Sumohana S
                  Channappayya
                  <br>
                  <em>In International Conference on Image Processing (ICIP)</em>, 2018
                  <br>
                  <a href="javascript:toggleBlock('abs_icip')">abstract</a>
                  /
                  <a href="javascript:toggleBlock('bib_icip')">bibtex</a>
                  /
                  <a href="https://arxiv.org/pdf/1711.07245.pdf">paper</a>
                  /
                  <a href="https://github.com/srikarym/OCR_Telugu_code">code</a>
                  <div id="abs_icip">
                    <p></p>
                    Telugu is a Dravidian language spoken by more than 80 million people worldwide. The optical
                    character recognition (OCR) of the Telugu script has wide ranging applications including education,
                    health-care, administration etc. The beautiful Telugu script however is very different from Germanic
                    scripts like English and German. This makes the use of transfer learning of Germanic OCR solutions
                    to Telugu a non-trivial task. To address the challenge of OCR for Telugu, we make three
                    contributions in this work: (i) a database of Telugu characters, (ii) a deep learning based OCR
                    algorithm, and (iii) a client server solution for the online deployment of the algorithm. For the
                    benefit of the Telugu people and the research community, our code has been made freely available at
                    this link.
                    <p></p>
                  </div>
                  <div id="bib_icip">
                    <break-spaces>
                      <p></p>
                      @inproceedings{prakash2018optical,<br>
                      title={Optical character recognition (ocr) for telugu: Database, algorithm and application},<br>
                      author={Prakash, Konkimalla Chandra and Srikar, YM and Trishal, Gayam and Mandal, Souraj and
                      Channappayya, Sumohana S},<br>
                      booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},<br>
                      pages={3963--3967},<br>
                      year={2018},<br>
                      organization={IEEE}
                      }
                      <p></p>
                    </break-spaces>
                  </div>

                </td>
              </tr>

            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">

                    <a href="https://jonbarron.info/">Credits</a>

                    <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <script>
            function toggleBlock(id) {
              var x = document.getElementById(id);
              if (x.style.display === "none") {
                x.style.display = "flex";
              } else {
                x.style.display = "none";
              }
            }
          </script>
          <script>
            toggleBlock('abs_arxiv_12_12');
            toggleBlock('bib_arxiv_12_12');
            toggleBlock('abs_arxiv_9_23');
            toggleBlock('bib_arxiv_9_23');
            toggleBlock('abs_arxiv_6_23');
            toggleBlock('bib_arxiv_6_23');
            toggleBlock('abs_arxiv_3_23');
            toggleBlock('bib_arxiv_3_23');
            toggleBlock('abs_thesis');
            toggleBlock('bib_thesis');
            toggleBlock('abs_isbi');
            toggleBlock('bib_isbi');
            toggleBlock('abs_midl');
            toggleBlock('bib_midl');
            toggleBlock('abs_icip');
            toggleBlock('bib_icip');
          </script>
</body>

</html>